{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IdcOawKfYk7"
   },
   "source": [
    "# Part 1: Setup and Data Pre-Preprocessing\r\n",
    "Note: To run this notebook, you need to create a folder in the same folder as this notebook, called \"sample_data\". This \"sample_data\" folder should have the \"proj1.csv\" and \"proj1_no_headers.csv\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6xYTtc-3t8qs"
   },
   "outputs": [],
   "source": [
    "from typing import *  # type: ignore\n",
    "\n",
    "import pyspark  # type: ignore\n",
    "from pyspark.rdd import RDD, PipelinedRDD  # type: ignore\n",
    "\n",
    "sc: pyspark.SparkContext = pyspark.SparkContext(appName=\"relevantAnswerClassifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvyJz9kqfn2x"
   },
   "source": [
    "## Build RDD (Resilient Distributed Datasets) From Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3eec69b7-4d94-4bdb-bd53-9d7656ee35f8",
     "showTitle": false,
     "title": ""
    },
    "id": "w-7TJEBgsRSN"
   },
   "outputs": [],
   "source": [
    "fileName = \"sample_data/proj1_no_headers.csv\"\n",
    "\n",
    "SOF_RDD: RDD = sc.textFile(fileName, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de8abeb9-c6c9-4a39-8b69-787f189dc859",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcDsB18PsRSO",
    "outputId": "f78596c5-2deb-46b9-d01f-fc8e44495bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in dataset:  263540\n",
      "Data of sample point:  ['563355,62701,0,1235000081,\"php,error,gd,image-processing\",220,2,563372,67183,2,1235000501']\n"
     ]
    }
   ],
   "source": [
    "numPoints: int = SOF_RDD.count()\n",
    "print(\"Number of points in dataset: \", numPoints)\n",
    "samplePoints: List[Any] = SOF_RDD.take(1)\n",
    "print(\"Data of sample point: \", samplePoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RZ1DLLEf7GI"
   },
   "source": [
    "## Data Description\r\n",
    "\r\n",
    "## Description of the Original Dataset\r\n",
    "|Column|Description|\r\n",
    "|---|---|\r\n",
    "|qid|Unique question id|\r\n",
    "|id|User id of questioner|\r\n",
    "|qs|Score of the question|\r\n",
    "|qt|Time of the question (in epoch time)|\r\n",
    "|tags|Comma-separated list of the tags associated with the question. Examples of tags are ``html'', ``R'', ``mysql'', ``python'', and so on; often between two and six tags are used on each question.|\r\n",
    "|qvc|Number of views of this question (at the time of the datadump)|\r\n",
    "|qac|Number of answers for this question (at the time of the datadump)|\r\n",
    "|aid|Unique answer id|\r\n",
    "|j|User id of answerer|\r\n",
    "|as|Score of the answer|\r\n",
    "|at|Time of the answer|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qR_aoqKXvPMf",
    "outputId": "ce47786a-a375-498e-ab17-caef183dee46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_data/proj1_no_headers.csv MapPartitionsRDD[8] at textFile at <unknown>:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOF_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tok8MhoHiwNk"
   },
   "source": [
    "## Create Dictionary Which Assigns Unique IDs to Each Combination of Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qPvyBA3qOU-N"
   },
   "outputs": [],
   "source": [
    "def get_tags(SOF_RDDRecord: str) -> str:\n",
    "    \"\"\"\n",
    "    Joins tags with a character for later assigning a unique ID to each unique combination of tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    SOF_RDDRecord : str\n",
    "        A record of an answer to a question on Stack Overflow.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The record of the answer, but with each tag joined together by a character.\n",
    "    \"\"\"\n",
    "\n",
    "    fieldsList: List[Any] = SOF_RDDRecord.split(\",\")\n",
    "\n",
    "    index: int = 4\n",
    "\n",
    "    tags: str = \"\"\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        if fieldsList[index].replace(\".\", \"\", 1).isdigit():\n",
    "\n",
    "            tags = tags[:-1]\n",
    "\n",
    "            break\n",
    "\n",
    "        tags += fieldsList[index] + \"_\"\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Create dictionary of unique IDs for each unique combination of tags.\n",
    "dictionary: RDD = SOF_RDD.map(get_tags).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tl_BsTji68h"
   },
   "source": [
    "## Create RDD With First Field as Label and Subsequent Fields as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf669c76-8628-4150-8ef9-9ee83ba4d1f0",
     "showTitle": false,
     "title": ""
    },
    "id": "8_hi47EHsRSP"
   },
   "outputs": [],
   "source": [
    "def map_SOF(SOF_RDDRecord: str) -> str:\n",
    "    \"\"\"\n",
    "    Pre-processes and performs feature engineering on a raw record of an answer to a Stack Overflow question.\n",
    "\n",
    "    The pre-procecssing only keeps relevant features, and the feature engineering transforms raw features into new features.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    SOF_RDDRecord : str\n",
    "        The raw record of an answer to a Stack Overflow question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The pre-processesed and feature engineered record of the provided answer to a Stack Overflow question.\n",
    "    \"\"\"\n",
    "\n",
    "    fieldsList: List[Any] = SOF_RDDRecord.split(\",\")\n",
    "    # qid = fieldsList[2]\n",
    "    qs: str = fieldsList[2]  # score of question\n",
    "    qt: str = fieldsList[3]  # time of the question\n",
    "    tags: str = \"\"\n",
    "\n",
    "    index: int = 4\n",
    "    while 1:\n",
    "        if fieldsList[index].replace(\".\", \"\", 1).isdigit():\n",
    "            tags = tags[:-1]\n",
    "            break\n",
    "        tags += fieldsList[index] + \"_\"\n",
    "        index += 1\n",
    "    num_tags: int = index - 4  # number of tags\n",
    "    qvc: str = fieldsList[index]  # number of views of this question\n",
    "    qac: str = fieldsList[index + 1]  # number of answers for this question\n",
    "    aid: str = fieldsList[index + 2]  # unique answer id\n",
    "    j: str = fieldsList[index + 3]  # User id of answerer\n",
    "    ans_score: str = fieldsList[index + 4]  # score of the answer\n",
    "    at: str = fieldsList[index + 5]  # time of the answer\n",
    "\n",
    "    isAnswered: str = \"0\"\n",
    "    if int(ans_score) > 0:\n",
    "        isAnswered = \"1\"\n",
    "\n",
    "    deltaTimeQToA: int = abs(int(qt) - int(at))\n",
    "    tag_indexer = str(dictionary[tags])\n",
    "    result: str = \",\".join(\n",
    "        [isAnswered, j, qs, qvc, qac, str(deltaTimeQToA), tag_indexer]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "SOF_RDD_BinAnsScor: RDD = SOF_RDD.map(map_SOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcfROPzLjmPj"
   },
   "source": [
    "## Filter Non-Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JTARCQ272Laj"
   },
   "outputs": [],
   "source": [
    "num_indices: List[int] = [0, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "\n",
    "def filter_non_numeric(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns true if all features of the record provided at the numeric indices are indeed numeric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line : str\n",
    "        The raw record of the RDD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if specified features are numeric, False otherwise.\n",
    "    \"\"\"\n",
    "    line_lst: List[str] = [val.strip() for val in line.split(\",\")]\n",
    "    for i in num_indices:\n",
    "        if not line_lst[i].replace(\".\", \"\", 1).isdigit():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "SOF_RDD_BinAnsScor = SOF_RDD.map(map_SOF).filter(filter_non_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba921488-70a1-4488-b4bf-36e037a2d749",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7H-MI0GIsRSP",
    "outputId": "0e386c93-19f6-41ce-91bf-1ca3eb6bb719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few filtered points:  ['1,67183,0,220,2,420,49250', '0,66554,0,220,2,470,49250', '1,15842,10,1047,16,37,41190', '1,893,10,1047,16,1405,41190', '1,11649,10,1047,16,2317,41190', '1,50742,10,1047,16,2669,41190', '1,8899,10,1047,16,3126,41190', '1,60190,10,1047,16,7677,41190', '1,65235,10,1047,16,7773,41190', '1,32797,10,1047,16,20486,41190']\n",
      "Number of filtered points:  258488\n"
     ]
    }
   ],
   "source": [
    "print(\"First few filtered points: \", SOF_RDD_BinAnsScor.take(10))\n",
    "\n",
    "print(\"Number of filtered points: \", SOF_RDD_BinAnsScor.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htWZCDrUjy9U"
   },
   "source": [
    "## Parse RDD Into RDD of Labeled Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9bd8f93-b9ca-474b-b987-5a98033c2ba6",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M0WnK7W-sRSP",
    "outputId": "248bc52c-a89c-4bca-e301-3d23319fecfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First point:  (1.0,[67183.0,0.0,220.0,2.0,420.0,49250.0])\n",
      "First point features:  [67183.0,0.0,220.0,2.0,420.0,49250.0]\n",
      "First point label:  1.0\n",
      "Number of first point features:  6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # type: ignore\n",
    "from pyspark.mllib.linalg import DenseVector  # type: ignore\n",
    "from pyspark.mllib.regression import LabeledPoint  # type: ignore\n",
    "\n",
    "\n",
    "def parsePoint(line: str) -> LabeledPoint:\n",
    "    \"\"\"\n",
    "    Parses a record of features and its label into a Labeled Point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line : str\n",
    "        The raw record of features and its label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LabeledPoint\n",
    "        The Labeled Point representing the record of the answer to a Stack Overflow question.\n",
    "    \"\"\"\n",
    "    label_features: List[str] = line.split(\",\")\n",
    "    ret_val: LabeledPoint = LabeledPoint(label_features[0], label_features[1:])\n",
    "    return ret_val\n",
    "\n",
    "\n",
    "parsedSamplePoints: RDD = SOF_RDD_BinAnsScor.map(parsePoint)\n",
    "\n",
    "firstPoint: LabeledPoint = parsedSamplePoints.take(1)[0]\n",
    "firstPointFeatures: DenseVector = firstPoint.features\n",
    "firstPointLabel: float = firstPoint.label\n",
    "d: int = len(firstPointFeatures)\n",
    "\n",
    "print(\"First point: \", firstPoint)\n",
    "print(\"First point features: \", firstPointFeatures)\n",
    "print(\"First point label: \", firstPointLabel)\n",
    "print(\"Number of first point features: \", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuiVYfe64X3L"
   },
   "source": [
    "## Apply Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "228e8561-d1ef-43e4-b987-93b8f537e6ac",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceXEUncysRSP",
    "outputId": "4a0945b3-0509-403b-9070-6f5ba98b0c7c"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-52c128c825bd>, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-52c128c825bd>\"\u001b[1;36m, line \u001b[1;32m51\u001b[0m\n\u001b[1;33m    global broadcastMean: float\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.broadcast import Broadcast  # type: ignore\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "def normalizeFeatures(lp: LabeledPoint) -> LabeledPoint:\n",
    "    \"\"\"\n",
    "    Normalizes a labeled point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lp : Labeled Point\n",
    "        The unnormalized labeled point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LabeledPoint\n",
    "        The normalized labeled point.\n",
    "    \"\"\"\n",
    "    normalizedFeatures: List[float] = list()\n",
    "    for i in range(0, len(lp.features)):\n",
    "        feature: float = (lp.features[i] - broadcastMean.value[i]) / broadcastStdev.value[i]  # type: ignore\n",
    "        normalizedFeatures.insert(i, feature)\n",
    "    return LabeledPoint(lp.label, normalizedFeatures)\n",
    "\n",
    "\n",
    "def getNormalizedRDD(nonNormalizedRDD: RDD) -> RDD:\n",
    "    \"\"\"\n",
    "    Normalizes the labeled points of an RDD.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nonNormalizedRDD : RDD\n",
    "        The nonnormalized RDD of labeled points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    RDD\n",
    "        The normalized RDD of labeled points.\n",
    "    \"\"\"\n",
    "    meanList: List[float] = list()\n",
    "    stdevList: List[float] = list()\n",
    "    numFeatures: int = len(nonNormalizedRDD.take(1)[0].features)\n",
    "    for i in range(0, numFeatures):\n",
    "        featureRDD: RDD = nonNormalizedRDD.map(lambda lp: lp.features[i])\n",
    "        print(featureRDD)\n",
    "        featureMean: float = featureRDD.mean()\n",
    "        featureStdev: float = featureRDD.stdev()\n",
    "        meanList.insert(i, featureMean)\n",
    "        stdevList.insert(i, featureStdev)\n",
    "    global broadcastMean  # type: ignore\n",
    "    broadcastMean = sc.broadcast(meanList)  # type: ignore\n",
    "    global broadcastStdev  # type: ignore\n",
    "    broadcastStdev = sc.broadcast(stdevList)  # type: ignore\n",
    "    returnRDD: RDD = nonNormalizedRDD.map(normalizeFeatures)\n",
    "    return returnRDD\n",
    "\n",
    "\n",
    "normalizedSamplePoints: RDD = getNormalizedRDD(parsedSamplePoints)\n",
    "print(\"First few normalized sample points: \", normalizedSamplePoints.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQVMf2Va4EPz"
   },
   "source": [
    "## Split Dataset Into Training and Validation Datasets\n",
    "### Each Dataset Will Be Cached for Repeated Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2qnd7oi4FcZ",
    "outputId": "f2a91e9f-c281-49d7-aa46-55043c0e0d44"
   },
   "outputs": [],
   "source": [
    "weights: List[float] = [0.8, 0.2]  # train/test split\n",
    "seed: int = 42\n",
    "parsedTrainData: RDD\n",
    "parsedValData: RDD\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.randomSplit(weights, seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "nTrain: int = parsedTrainData.count()\n",
    "nVal: int = parsedValData.count()\n",
    "\n",
    "print(\"Number of Train Points: \", nTrain)\n",
    "print(\"Number of Validation Points: \", nVal)\n",
    "print(\"Number of Train and Validation Points: \", nTrain + nVal)\n",
    "print(\"Number of Normalized Sample Points: \", normalizedSamplePoints.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPdOXuOn4eb_"
   },
   "source": [
    "# Part 2: Create, Train, and Evaluate Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GSfxUc-49Fm"
   },
   "source": [
    "## Create Average (Mean) Label Baseline Model\n",
    "A very simple yet natural baseline model is one where we always make the same prediction independent of the given data point, using the average label in the training set as the constant prediction value. This value is rounded to the nearest integer to allow for binary classification (1 or 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpZhIt6J4fI5",
    "outputId": "4db9b650-72a5-4304-f4af-67ece5449668"
   },
   "outputs": [],
   "source": [
    "averageScore: float = int(round((parsedTrainData.map(lambda s: s.label)).mean()))\n",
    "print(\"Average (mean) score: \", averageScore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw3N7jSC92ZV"
   },
   "source": [
    "## Calculate Root Mean Squared Error (RMSE)\n",
    "Implement a function to compute RMSE given an RDD of (label, prediction) tuples, and test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0g5lWgGd4_ya"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RDD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-95c0e611773c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mcalcRMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelsAndPreds\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RDD' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Add acknowledgement that the squaredError and calcRMSE functions were created by Dr. Krishnamurthy\n",
    "import math\n",
    "\n",
    "\n",
    "def squaredError(label: float, prediction: float) -> float:\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "\n",
    "    :Authors:\n",
    "        Dr. Diwakar Krishnamurthy <dkrishna@ucalgary.ca>\n",
    "    \"\"\"\n",
    "    print(\"pre square error calc\")\n",
    "    sqrError: float = (label - prediction) * (label - prediction)\n",
    "    print(\"post square error calc\")\n",
    "    return sqrError\n",
    "\n",
    "\n",
    "def calcRMSE(labelsAndPreds: RDD) -> float:\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "\n",
    "    :Authors:\n",
    "        Dr. Diwakar Krishnamurthy <dkrishna@ucalgary.ca>\n",
    "    \"\"\"\n",
    "    sqrSum = labelsAndPreds.map(lambda s: squaredError(s[0], s[1])).sum()\n",
    "    return math.sqrt(sqrSum / labelsAndPreds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsSwEYVy99LG"
   },
   "source": [
    "## Create Evaluation Metrics Dataframes for Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pWLE-ov-Frr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # type: ignore\n",
    "\n",
    "\n",
    "def evaluationMetricsDataframe() -> pd.DataFrame:\n",
    "    \"\"\"Creates a Pandas DataFrame from storing evaluation metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The DataFrame of evaluation metrics.\n",
    "    \"\"\"\n",
    "    columns: List[str] = [\"RMSE\", \"Accuracy\", \"Precision\", \"Recall\", \"F_Score\"]\n",
    "\n",
    "    evaluationMetrics: pd.DataFrame = pd.DataFrame(\n",
    "        index=[\n",
    "            \"Baseline\",\n",
    "            \"Gradient_Boosted_Trees\",\n",
    "            \"SVM_With_SGD\",\n",
    "            \"Random_Forest\",\n",
    "            \"Logistic_Regression\",\n",
    "            \"Decision_Tree\",\n",
    "        ],\n",
    "        columns=columns,\n",
    "    )\n",
    "\n",
    "    return evaluationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "JL_S3J8xA1qc",
    "outputId": "2413eab5-8d63-44f0-8eb5-6e51a175a087"
   },
   "outputs": [],
   "source": [
    "evaluationMetricsTrain: pd.DataFrame = evaluationMetricsDataframe()\n",
    "\n",
    "evaluationMetricsTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "HnwHHuchA1tB",
    "outputId": "dad377da-0649-4357-8975-0aa12a352889"
   },
   "outputs": [],
   "source": [
    "evaluationMetricsVal: pd.DataFrame = evaluationMetricsDataframe()\n",
    "\n",
    "evaluationMetricsVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29KyF4kbnUiX"
   },
   "source": [
    "## Create RDD of Baseline Model Labels and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKwIx1Ay9_HP"
   },
   "outputs": [],
   "source": [
    "labelsAndPredsTrain: RDD = parsedTrainData.map(lambda s: (s.label, averageScore))\n",
    "labelsAndPredsVal: RDD = parsedValData.map(lambda s: (s.label, averageScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6f4dIEKnbw9"
   },
   "source": [
    "## Create Functions for Calculating Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRW_8le0tqWC"
   },
   "outputs": [],
   "source": [
    "def calcConfusionMatrixMetrics(\n",
    "    labelsAndPreds: RDD,\n",
    ") -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Calculates confusion matrix metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labelsAndPreds: RDD\n",
    "        The RDD of labels and predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, ...]\n",
    "        The tuple of confusion matrix metrics.\n",
    "    \"\"\"\n",
    "    # true positives\n",
    "    TP: int = labelsAndPreds.filter(lambda lp: (lp[0] == 1) and (lp[1] == 1)).count()\n",
    "    # true negatives\n",
    "    TN: int = labelsAndPreds.filter(lambda lp: (lp[0] == 0) and (lp[1] == 0)).count()\n",
    "    # false positives\n",
    "    FP: int = labelsAndPreds.filter(lambda lp: (lp[0] == 0) and (lp[1] == 1)).count()\n",
    "    # false negative\n",
    "    FN: int = labelsAndPreds.filter(lambda lp: (lp[0] == 1) and (lp[1] == 0)).count()\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rymLy3o6uGio"
   },
   "outputs": [],
   "source": [
    "def calcAccuracy(TP: int, TN: int, FP: int, FN: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : int\n",
    "        Number of true positives.\n",
    "    TN : int\n",
    "        Number of true negatives.\n",
    "    FP : int\n",
    "        Number of false positives.\n",
    "    FN : int\n",
    "        Number of false negatives.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Accuracy metric.\n",
    "    \"\"\"\n",
    "    accuracy: float = (TP + TN) / (TP + FP + TN + FN)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def calcPrecision(TP: int, FP: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates precision.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : int\n",
    "        Number of true positives.\n",
    "    FP : int\n",
    "        Number of false positives.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Precision metric.\n",
    "    \"\"\"\n",
    "    precision: float = TP / (TP + FP)\n",
    "    return precision\n",
    "\n",
    "\n",
    "def calcRecall(TP: int, FN: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculates precision.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : int\n",
    "        Number of true positives.\n",
    "    FN : int\n",
    "        Number of false negatives.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Recall metric.\n",
    "    \"\"\"\n",
    "    recall: float = (TP) / (TP + FN)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def calcFScore(precision: float, recall: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates f-score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision : float\n",
    "        Precision metric.\n",
    "    recall : float\n",
    "        Recall metric.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        F-Score metric.\n",
    "    \"\"\"\n",
    "    fScore: float = (2 * precision * recall) / (precision + recall)\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f848DrSn1wv"
   },
   "outputs": [],
   "source": [
    "# calculate confusion matrix metrics\n",
    "def calcEvaluationMetrics(\n",
    "    evaluationMetrics: pd.DataFrame, model_name: str, labelsAndPreds: RDD\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Calculates evaluation metrics for evaluation DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    evaluationMetrics : pd.DataFrame\n",
    "        The DataFrame of evaluation metrics.\n",
    "    labelsAndPreds : RDD\n",
    "        The RDD of labels and predictions.\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # RMSE\n",
    "    evaluationMetrics[\"RMSE\"][model_name] = calcRMSE(labelsAndPreds)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    TP: int\n",
    "    TN: int\n",
    "    FP: int\n",
    "    FN: int\n",
    "    TP, TN, FP, FN = calcConfusionMatrixMetrics(labelsAndPreds)\n",
    "\n",
    "    # Accuracy\n",
    "    evaluationMetrics[\"Accuracy\"][model_name] = calcAccuracy(TP, TN, FP, FN)\n",
    "\n",
    "    # Precision\n",
    "    evaluationMetrics[\"Precision\"][model_name] = calcPrecision(TP, FP)\n",
    "\n",
    "    # Recall\n",
    "    evaluationMetrics[\"Recall\"][model_name] = calcRecall(TP, FN)\n",
    "\n",
    "    # F-Score\n",
    "    evaluationMetrics[\"F_Score\"][model_name] = calcFScore(\n",
    "        evaluationMetrics[\"Precision\"][model_name],\n",
    "        evaluationMetrics[\"Recall\"][model_name],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNkeUbctnjeX"
   },
   "source": [
    "## Calculate Evaluation Metrics of Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRm7ea4BGZBv"
   },
   "outputs": [],
   "source": [
    "calcEvaluationMetrics(evaluationMetricsTrain, \"Baseline\", labelsAndPredsTrain)\n",
    "\n",
    "calcEvaluationMetrics(evaluationMetricsVal, \"Baseline\", labelsAndPredsVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "vJnpECOq-F0t",
    "outputId": "6ccc5ed9-a854-4353-a119-73d2d3e054a8"
   },
   "outputs": [],
   "source": [
    "print(\"Model Train Data Evaluation Metrics\")\n",
    "evaluationMetricsTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VRGG9EgM7b1"
   },
   "source": [
    "Explanation for recall being 1: The baseline model is just the mean value of the labels rounded to the nearest integer. In the case of this data set, that mean value rounded up to 1. Therefore, the baseline model will never have a false negative due it always classifying a sample as \"relevant\" (i.e., the label is always 1). This results in the Recall just being equivalent to TP (Number of true positives) over TP, which is just 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "rcLFil4ID8SI",
    "outputId": "7263251b-3da9-4ce0-b30b-6bf8dbe6cae0"
   },
   "outputs": [],
   "source": [
    "print(\"Model Validation Data Evaluation Metrics\")\n",
    "\n",
    "evaluationMetricsVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNijp8g-NadA"
   },
   "source": [
    "Explanation for recall being 1: We again see the baseline model has recall of 1, because it always classifies every sample as having a label of 1, thus there are no false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ej4jsCu-Mq6"
   },
   "source": [
    "# Part 3: Train Other Models by Using MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFe28Fgy-l2v"
   },
   "source": [
    "## Create and Fit Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryWb351e-ODV"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import (  # type: ignore\n",
    "    LogisticRegressionModel, LogisticRegressionWithLBFGS, SVMModel, SVMWithSGD)\n",
    "from pyspark.mllib.tree import DecisionTree  # type: ignore\n",
    "from pyspark.mllib.tree import (DecisionTreeModel, GradientBoostedTrees,\n",
    "                                GradientBoostedTreesModel, RandomForest,\n",
    "                                RandomForestModel)\n",
    "from pyspark.mllib.util import MLUtils  # type: ignore\n",
    "\n",
    "# Train a GradientBoostedTrees model.\n",
    "#  Notes: (a) Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#         (b) Using iterations.\n",
    "modelGradientBoostedTrees: GradientBoostedTrees = GradientBoostedTrees.trainClassifier(\n",
    "    parsedTrainData, categoricalFeaturesInfo={}, numIterations=3\n",
    ")\n",
    "\n",
    "SVMWithSGDModel: SVMWithSGD = SVMWithSGD.train(parsedTrainData, iterations=10)\n",
    "\n",
    "modelRandomForest: RandomForest = RandomForest.trainClassifier(\n",
    "    parsedTrainData,\n",
    "    categoricalFeaturesInfo={},\n",
    "    numTrees=8,\n",
    "    featureSubsetStrategy=\"auto\",\n",
    "    maxDepth=5,\n",
    "    maxBins=32,\n",
    "    numClasses=3,\n",
    ")\n",
    "\n",
    "modelLogisticRegression: LogisticRegressionWithLBFGS = (\n",
    "    LogisticRegressionWithLBFGS.train(parsedTrainData)\n",
    ")\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "decisionTree: DecisionTree = DecisionTree.trainClassifier(\n",
    "    parsedTrainData, categoricalFeaturesInfo={}, numClasses=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXzsIvFN-R4p"
   },
   "source": [
    "## Create RDD of Labels and Predictions of Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zq36m0DL-SgV",
    "outputId": "a1cd16f7-0697-4132-c3f1-bec5bc636ce9"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "samplePoint: LabeledPoint = parsedTrainData.take(1)[0]\n",
    "samplePrediction: float = modelGradientBoostedTrees.predict(samplePoint.features)\n",
    "\n",
    "print(\"samplePoint: \", samplePoint)\n",
    "print(\"sample label: \", samplePoint.label)\n",
    "print(\"sample features: \", samplePoint.features)\n",
    "print(\"sample prediction: \", samplePrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObKqO0Wp-v_X"
   },
   "source": [
    "## Calculate Evaluation Metrics of Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlUeHNpz-weO",
    "outputId": "b14d1378-62f9-4a4a-82e8-864e68a3e206"
   },
   "outputs": [],
   "source": [
    "MODEL_NAMES: List[str] = [\n",
    "    \"Gradient_Boosted_Trees\",\n",
    "    \"SVM_With_SGD\",\n",
    "    \"Random_Forest\",\n",
    "    \"Logistic_Regression\",\n",
    "    \"Decision_Tree\",\n",
    "]\n",
    "MODELS: list = [\n",
    "    modelGradientBoostedTrees,\n",
    "    SVMWithSGDModel,\n",
    "    modelRandomForest,\n",
    "    modelLogisticRegression,\n",
    "    decisionTree,\n",
    "]\n",
    "for modelName, model in zip(MODEL_NAMES, MODELS):\n",
    "    print(f\"{'Training and evaluating: ':<30}{modelName:>30}\")\n",
    "    predsTrain: RDD = model.predict(parsedTrainData.map(lambda x: x.features))\n",
    "    predsVal: RDD = model.predict(parsedValData.map(lambda x: x.features))\n",
    "\n",
    "    labelsAndPredsTrain = parsedTrainData.map(lambda lp: lp.label).zip(predsTrain)\n",
    "    labelsAndPredsVal = parsedValData.map(lambda lp: lp.label).zip(predsVal)\n",
    "\n",
    "    calcEvaluationMetrics(evaluationMetricsTrain, modelName, labelsAndPredsTrain)\n",
    "    calcEvaluationMetrics(evaluationMetricsVal, modelName, labelsAndPredsVal)\n",
    "    print(f\"{'Done training and evaluating: ':<30}{modelName:>30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "6s3C4NAEG8_X",
    "outputId": "28c33152-8a4f-495f-e7ad-937aaf46e8c0"
   },
   "outputs": [],
   "source": [
    "print(\"Model Training Data Evaluation Metrics\")\n",
    "\n",
    "evaluationMetricsTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "W9EGqrAeqK7a",
    "outputId": "d63a326c-52b5-43c5-8b5a-cb8003a90744"
   },
   "outputs": [],
   "source": [
    "print(\"Model Validation Data Evaluation Metrics\")\n",
    "evaluationMetricsVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhQtsRgRojqx"
   },
   "source": [
    "## Visualizations of Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "i5IRXWT-ArJD",
    "outputId": "c82d0470-c6c9-4917-c047-0a99ce71b04d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "\n",
    "fig = evaluationMetricsTrain.T.plot.bar(figsize=(24, 12), rot=45)\n",
    "\n",
    "fig.set_title(\n",
    "    \"Training Data: Evaluation Metric Value vs. Evaluation Metric Type\",\n",
    "    fontsize=40,\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "fig.set_xlabel(\"Evaluation Metric Type\", fontsize=25)\n",
    "\n",
    "fig.set_ylabel(\"Evaluation Metric Value\", fontsize=25)\n",
    "\n",
    "fig.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=20)\n",
    "\n",
    "plt.xticks(size=20)\n",
    "\n",
    "plt.yticks(size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "X7K0J5brDh49",
    "outputId": "13c38af1-e62f-471d-a06b-484ddf1fa206"
   },
   "outputs": [],
   "source": [
    "fig = evaluationMetricsVal.T.plot.bar(figsize=(24, 12), rot=45)\n",
    "\n",
    "fig.set_title(\n",
    "    \"Validation Data: Evaluation Metric Value vs. Evaluation Metric Type\",\n",
    "    fontsize=40,\n",
    "    pad=20,\n",
    ")\n",
    "\n",
    "fig.set_xlabel(\"Evaluation Metric Type\", fontsize=25)\n",
    "\n",
    "fig.set_ylabel(\"Evaluation Metric Value\", fontsize=25)\n",
    "\n",
    "fig.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=20)\n",
    "\n",
    "plt.xticks(size=20)\n",
    "\n",
    "plt.yticks(size=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JBE6BroLC8pB",
    "outputId": "5dc56abb-f941-4129-9d5f-f8b192e5227f"
   },
   "outputs": [],
   "source": [
    "for col in evaluationMetricsTrain.columns:\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    fig1 = evaluationMetricsTrain.sort_values(by=[col])[col].plot.bar(\n",
    "        figsize=(24, 12), rot=45, ax=axes[0]\n",
    "    )\n",
    "\n",
    "    fig1.set_title(f\"Training Data: \\n{col} vs. Model\", fontsize=40, pad=20)\n",
    "\n",
    "    fig1.set_ylabel(f\"{col}\", fontsize=25)\n",
    "\n",
    "    fig1.set_xlabel(\"Model\", fontsize=25)\n",
    "\n",
    "    fig1.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=20)\n",
    "\n",
    "    plt.sca(axes[0])\n",
    "\n",
    "    plt.xticks(size=15)\n",
    "\n",
    "    plt.yticks(size=15)\n",
    "\n",
    "    fig2 = evaluationMetricsVal.sort_values(by=[col])[col].plot.bar(\n",
    "        figsize=(24, 12), rot=45, ax=axes[1]\n",
    "    )\n",
    "\n",
    "    fig2.set_title(f\"Validation Data: \\n{col} vs. Model\", fontsize=40, pad=20)\n",
    "\n",
    "    fig2.set_ylabel(f\"{col}\", fontsize=25)\n",
    "\n",
    "    fig2.set_xlabel(\"Model\", fontsize=25)\n",
    "\n",
    "    fig2.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=20)\n",
    "\n",
    "    plt.sca(axes[1])\n",
    "\n",
    "    plt.xticks(size=15)\n",
    "\n",
    "    plt.yticks(size=15)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "SENG550_Project1",
   "notebookOrigID": 1651283655326682,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "SENG550_Project_PySpark_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
